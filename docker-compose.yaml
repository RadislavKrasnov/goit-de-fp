version: "3.9"

services:
  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@localhost:9093
      - ALLOW_PLAINTEXT_LISTENER=yes
    volumes:
      - kafka_data:/bitnami/kafka

  spark-client:
    build:
      context: .
      dockerfile_inline: |
        FROM python:3.12-slim

        # Install tools + Java for Spark
        RUN apt-get update && apt-get install -y wget gnupg apt-transport-https curl procps iputils-ping && rm -rf /var/lib/apt/lists/*
        RUN wget -O- https://packages.adoptium.net/artifactory/api/gpg/key/public | gpg --dearmor | tee /usr/share/keyrings/adoptium.gpg \
        && echo "deb [signed-by=/usr/share/keyrings/adoptium.gpg] https://packages.adoptium.net/artifactory/deb jammy main" > /etc/apt/sources.list.d/adoptium.list \
        && apt-get update \
        && apt-get install -y iputils-ping \
        && apt-get install -y temurin-17-jdk \
        && rm -rf /var/lib/apt/lists/*
        ENV JAVA_HOME=/usr/lib/jvm/temurin-17-jdk-amd64
        ENV PATH="${JAVA_HOME}/bin:${PATH}"

        # Download Spark 4.0.0 Kafka connector + all dependencies for Scala 2.13
        RUN mkdir -p /opt/spark/jars && \
          wget -q https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/4.0.0/spark-sql-kafka-0-10_2.13-4.0.0.jar -P /opt/spark/jars && \
          wget -q https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.12.0/commons-pool2-2.12.0.jar -P /opt/spark/jars

        # Install PySpark and kafka-python
        RUN pip install --no-cache-dir pyspark==4.0.0 kafka-python python-dotenv

        WORKDIR /app
        CMD ["tail", "-f", "/dev/null"]
    container_name: spark-client
    volumes:
      - ./:/app
    depends_on:
      - kafka
    ports:
      - "4040:4040"

volumes:
  kafka_data:
